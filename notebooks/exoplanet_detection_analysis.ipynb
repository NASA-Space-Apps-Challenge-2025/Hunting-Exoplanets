{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6140b0f5",
   "metadata": {},
   "source": [
    "## Data Sources and Citations\n",
    "\n",
    "### Primary Data Source\n",
    "**NASA Exoplanet Archive** - The primary dataset used in this analysis comes from the NASA Exoplanet Archive, which is operated by the California Institute of Technology, under contract with the National Aeronautics and Space Administration under the Exoplanet Exploration Program.\n",
    "\n",
    "### Citations\n",
    "\n",
    "#### NASA Exoplanet Archive\n",
    "- **Citation**: Akeson, R. L., Chen, X., Ciardi, D., et al. 2013, PASP, 125, 989\n",
    "- **DOI**: [10.1086/672273](https://doi.org/10.1086/672273)\n",
    "- **Website**: [https://exoplanetarchive.ipac.caltech.edu/](https://exoplanetarchive.ipac.caltech.edu/)\n",
    "- **Access Date**: October 2025\n",
    "\n",
    "#### Planetary Systems Composite Parameters Table\n",
    "- **Table**: `ps` (Planetary Systems Composite Parameters)\n",
    "- **Description**: Contains orbital and physical parameters for confirmed exoplanets and their host stars\n",
    "- **URL**: https://exoplanetarchive.ipac.caltech.edu/cgi-bin/TblView/nph-tblView?app=ExoTbls&config=PS\n",
    "\n",
    "#### Data Attribution\n",
    "This research has made use of the NASA Exoplanet Archive, which is operated by the California Institute of Technology, under contract with the National Aeronautics and Space Administration under the Exoplanet Exploration Program.\n",
    "\n",
    "### Acknowledgments\n",
    "We acknowledge the dedicated work of the international astronomical community in discovering and characterizing exoplanets, particularly:\n",
    "- **Kepler Space Telescope Mission** - For transit photometry data\n",
    "- **TESS (Transiting Exoplanet Survey Satellite)** - For continued exoplanet discoveries\n",
    "- **Radial Velocity Surveys** - Including HARPS, HIRES, and other ground-based programs\n",
    "- **Direct Imaging Programs** - For imaging exoplanets around nearby stars\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af014db",
   "metadata": {},
   "source": [
    "# Exoplanet Detection Using Machine Learning\n",
    "\n",
    "## Project Overview\n",
    "This notebook implements a comprehensive machine learning approach for exoplanet detection and classification, following research-based methodologies. We'll explore data from NASA's Exoplanet Archive, preprocess the features, and evaluate multiple classification algorithms to identify the most effective approach for exoplanet detection.\n",
    "\n",
    "## Objectives\n",
    "1. **Data Exploration**: Load and examine exoplanet datasets from NASA archives\n",
    "2. **Data Preprocessing**: Clean, scale, and prepare features for machine learning\n",
    "3. **Model Training**: Implement and compare multiple classification algorithms\n",
    "4. **Performance Evaluation**: Assess models using scientific validation metrics\n",
    "5. **Feature Analysis**: Understand which features are most important for detection\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3879a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_exoplanet_data():\n",
    "    \"\"\"Load exoplanet data from NASA Exoplanet Archive using TAP API with cumulative table (KOI cumulative delivery)\"\"\"\n",
    "    \n",
    "    try:\n",
    "        import requests\n",
    "        from io import StringIO\n",
    "        import pandas as pd\n",
    "        \n",
    "        print(\"üåå Fetching exoplanet data from NASA Exoplanet Archive (TAP API)...\")\n",
    "        print(\"üìä Using cumulative table (KOI cumulative delivery)\")\n",
    "        \n",
    "        # Use TAP API with cumulative table\n",
    "        tap_url = \"https://exoplanetarchive.ipac.caltech.edu/TAP/sync\"\n",
    "        \n",
    "        # Query for KOI cumulative data with reasonable limit\n",
    "        query = \"\"\"\n",
    "        SELECT TOP 10000 * FROM cumulative \n",
    "        WHERE koi_disposition IS NOT NULL\n",
    "        ORDER BY kepoi_name\n",
    "        \"\"\"\n",
    "        \n",
    "        params = {\n",
    "            'query': query.strip(),\n",
    "            'format': 'csv'\n",
    "        }\n",
    "        \n",
    "        print(f\"üîç Executing query: {query.strip()}\")\n",
    "        print(f\"üì° API URL: {tap_url}\")\n",
    "        \n",
    "        response = requests.get(tap_url, params=params, timeout=60)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            df = pd.read_csv(StringIO(response.text))\n",
    "            print(f\"‚úÖ Successfully loaded {len(df)} KOI records from cumulative table!\")\n",
    "            print(f\"üìä Data shape: {df.shape}\")\n",
    "            print(f\"üè∑Ô∏è  Columns: {len(df.columns)}\")\n",
    "            return df\n",
    "        else:\n",
    "            print(f\"‚ùå TAP API failed with status code: {response.status_code}\")\n",
    "            print(f\"Response: {response.text[:500]}...\")\n",
    "            \n",
    "    except ImportError as e:\n",
    "        print(f\"‚ùå Missing required library: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error fetching data: {e}\")\n",
    "    \n",
    "    print(\"\\n‚ö†Ô∏è  Unable to fetch real NASA data. This could be due to:\")\n",
    "    print(\"   1. NASA server maintenance or temporary issues\")\n",
    "    print(\"   2. Network connectivity problems\")\n",
    "    print(\"   3. API endpoint changes\")\n",
    "    print(\"   4. Rate limiting or access restrictions\")\n",
    "    print(\"\\nWould you like to:\")\n",
    "    print(\"   A) Try again later when NASA servers may be available\")\n",
    "    print(\"   B) Use a local backup dataset if available\")\n",
    "    print(\"   C) Continue with sample data for demonstration\")\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1836f4d",
   "metadata": {},
   "source": [
    "## 1. Load and Explore Exoplanet Dataset\n",
    "\n",
    "We'll start by loading the exoplanet data from NASA's Exoplanet Archive. This dataset contains information about confirmed exoplanets including their discovery methods, orbital characteristics, and physical properties.\n",
    "\n",
    "### Data Source Attribution\n",
    "This analysis uses data from the **NASA Exoplanet Archive**, operated by the California Institute of Technology under contract with NASA. The primary dataset comes from the Planetary Systems Composite Parameters table, which aggregates orbital and physical parameters for confirmed exoplanets and their host stars.\n",
    "\n",
    "**Citation**: Akeson, R. L., Chen, X., Ciardi, D., et al. 2013, PASP, 125, 989  \n",
    "**Website**: https://exoplanetarchive.ipac.caltech.edu/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4a8ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Exoplanet Dataset from NASA Exoplanet Archive\n",
    "def load_exoplanet_data():\n",
    "    \"\"\"\n",
    "    Load exoplanet data from NASA Exoplanet Archive using TAP API\n",
    "    \n",
    "    Data Source: NASA Exoplanet Archive\n",
    "    Citation: Akeson, R. L., Chen, X., Ciardi, D., et al. 2013, PASP, 125, 989\n",
    "    Website: https://exoplanetarchive.ipac.caltech.edu/\n",
    "    \n",
    "    This research has made use of the NASA Exoplanet Archive, which is operated \n",
    "    by the California Institute of Technology, under contract with the National \n",
    "    Aeronautics and Space Administration under the Exoplanet Exploration Program.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        import requests\n",
    "        print(\"üåå Loading exoplanet data from NASA Exoplanet Archive using TAP API...\")\n",
    "        \n",
    "        # TAP API query for Kepler Objects of Interest (KOI) cumulative table\n",
    "        tap_url = \"https://exoplanetarchive.ipac.caltech.edu/TAP/sync\"\n",
    "        query = \"\"\"\n",
    "        SELECT *\n",
    "        FROM cumulative \n",
    "        \"\"\"\n",
    "        \n",
    "        params = {\n",
    "            'query': query.strip(),\n",
    "            'format': 'csv'\n",
    "        }\n",
    "        \n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (compatible; ExoplanetAnalysis/1.0)',\n",
    "            'Accept': 'text/csv,application/csv'\n",
    "        }\n",
    "        \n",
    "        response = requests.get(tap_url, params=params, headers=headers, timeout=30)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            from io import StringIO\n",
    "            df = pd.read_csv(StringIO(response.text))\n",
    "            \n",
    "            if len(df) > 0:\n",
    "                print(f\"‚úÖ Successfully loaded {len(df)} exoplanet records from NASA!\")\n",
    "                print(f\"   Columns: {list(df.columns)[:5]}... (showing first 5)\")\n",
    "                print(f\"   Disposition distribution: {df['koi_disposition'].value_counts().to_dict()}\")\n",
    "                return df\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è  Query returned no data\")\n",
    "                return None\n",
    "        else:\n",
    "            print(f\"‚ùå TAP API request failed with status code: {response.status_code}\")\n",
    "            print(f\"   Response: {response.text[:200]}...\")\n",
    "            return None\n",
    "            \n",
    "    except ImportError:\n",
    "        print(\"‚ùå Requests library not available. Please install with: pip install requests\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading data from NASA Exoplanet Archive: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9399a48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "exoplanet_data = load_exoplanet_data()\n",
    "\n",
    "if exoplanet_data is not None:\n",
    "    print(f\"\\nüìä Dataset Overview:\")\n",
    "    print(f\"Shape: {exoplanet_data.shape}\")\n",
    "    print(f\"Columns: {exoplanet_data.shape[1]}\")\n",
    "    print(f\"Rows: {exoplanet_data.shape[0]}\")\n",
    "    print(f\"Column names: {list(exoplanet_data.columns)}\")\n",
    "else:\n",
    "    print(\"Failed to load exoplanet data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2467bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive Dataset Exploration and Analysis\n",
    "print(\"\udd2c COMPREHENSIVE EXOPLANET DATASET EXPLORATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if exoplanet_data is not None:\n",
    "    # 1. BASIC DATASET OVERVIEW\n",
    "    print(\"üìä 1. BASIC DATASET OVERVIEW\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"Dataset shape: {exoplanet_data.shape}\")\n",
    "    print(f\"Memory usage: {exoplanet_data.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    print(f\"Data types: {exoplanet_data.dtypes.value_counts().to_dict()}\")\n",
    "    \n",
    "    # Column categorization\n",
    "    numerical_cols = exoplanet_data.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    categorical_cols = exoplanet_data.select_dtypes(include=['object']).columns.tolist()\n",
    "    \n",
    "    print(f\"Numerical features: {len(numerical_cols)}\")\n",
    "    print(f\"Categorical features: {len(categorical_cols)}\")\n",
    "    \n",
    "    # 2. SAMPLE DATA INSPECTION\n",
    "    print(f\"\\nüìã 2. SAMPLE DATA INSPECTION\")\n",
    "    print(\"-\" * 40)\n",
    "    print(\"First 5 rows:\")\n",
    "    display(exoplanet_data.head())\n",
    "    \n",
    "    print(\"\\nRandom sample of 5 rows:\")\n",
    "    display(exoplanet_data.sample(5, random_state=42))\n",
    "    \n",
    "    # 3. COMPREHENSIVE STATISTICAL SUMMARY\n",
    "    print(f\"\\nüìà 3. STATISTICAL SUMMARY\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Numerical features summary\n",
    "    if numerical_cols:\n",
    "        print(\"Numerical Features Summary:\")\n",
    "        desc_stats = exoplanet_data[numerical_cols].describe()\n",
    "        display(desc_stats)\n",
    "        \n",
    "        # Additional statistics\n",
    "        print(\"\\nAdditional Numerical Statistics:\")\n",
    "        additional_stats = exoplanet_data[numerical_cols].agg(['skew', 'kurtosis']).T\n",
    "        additional_stats.columns = ['Skewness', 'Kurtosis']\n",
    "        display(additional_stats)\n",
    "    \n",
    "    # Categorical features summary\n",
    "    if categorical_cols:\n",
    "        print(\"\\nCategorical Features Summary:\")\n",
    "        for col in categorical_cols[:5]:  # Show first 5 categorical columns\n",
    "            if exoplanet_data[col].nunique() < 20:  # Only show if not too many unique values\n",
    "                print(f\"\\n{col} distribution:\")\n",
    "                value_counts = exoplanet_data[col].value_counts()\n",
    "                display(value_counts.head(10))\n",
    "    \n",
    "    # 4. MISSING VALUES ANALYSIS\n",
    "    print(f\"\\nüîç 4. MISSING VALUES ANALYSIS\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    missing_data = exoplanet_data.isnull().sum()\n",
    "    missing_percent = (missing_data / len(exoplanet_data)) * 100\n",
    "    \n",
    "    missing_df = pd.DataFrame({\n",
    "        'Missing_Count': missing_data,\n",
    "        'Missing_Percentage': missing_percent\n",
    "    }).sort_values('Missing_Percentage', ascending=False)\n",
    "    \n",
    "    missing_df = missing_df[missing_df['Missing_Count'] > 0]\n",
    "    \n",
    "    if not missing_df.empty:\n",
    "        print(\"Features with missing values:\")\n",
    "        display(missing_df)\n",
    "        \n",
    "        # Missing values heatmap\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.heatmap(exoplanet_data.isnull(), cbar=False, cmap='viridis')\n",
    "        plt.title('Missing Values Heatmap')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Missing values by percentage ranges\n",
    "        missing_ranges = {\n",
    "            '0-10%': len(missing_df[missing_df['Missing_Percentage'] <= 10]),\n",
    "            '10-25%': len(missing_df[(missing_df['Missing_Percentage'] > 10) & (missing_df['Missing_Percentage'] <= 25)]),\n",
    "            '25-50%': len(missing_df[(missing_df['Missing_Percentage'] > 25) & (missing_df['Missing_Percentage'] <= 50)]),\n",
    "            '>50%': len(missing_df[missing_df['Missing_Percentage'] > 50])\n",
    "        }\n",
    "        print(\"\\nMissing values distribution by percentage ranges:\")\n",
    "        for range_name, count in missing_ranges.items():\n",
    "            print(f\"  {range_name}: {count} features\")\n",
    "    else:\n",
    "        print(\"‚úÖ No missing values found in the dataset\")\n",
    "    \n",
    "    # 5. CORRELATION ANALYSIS\n",
    "    print(f\"\\nüîó 5. CORRELATION ANALYSIS\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    if len(numerical_cols) > 1:\n",
    "        # Correlation matrix for numerical features\n",
    "        corr_matrix = exoplanet_data[numerical_cols].corr()\n",
    "        \n",
    "        # Plot correlation heatmap\n",
    "        plt.figure(figsize=(14, 12))\n",
    "        mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "        sns.heatmap(corr_matrix, mask=mask, annot=False, cmap='coolwarm', center=0,\n",
    "                   square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "        plt.title('Feature Correlation Matrix')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Top correlations\n",
    "        print(\"\\nTop 10 strongest correlations:\")\n",
    "        corr_pairs = []\n",
    "        for i in range(len(corr_matrix.columns)):\n",
    "            for j in range(i+1, len(corr_matrix.columns)):\n",
    "                corr_pairs.append((corr_matrix.columns[i], corr_matrix.columns[j], corr_matrix.iloc[i, j]))\n",
    "        \n",
    "        corr_pairs.sort(key=lambda x: abs(x[2]), reverse=True)\n",
    "        for col1, col2, corr in corr_pairs[:10]:\n",
    "            print(f\"  {col1} ‚Üî {col2}: {corr:.3f}\")\n",
    "    \n",
    "    # 6. DISTRIBUTION ANALYSIS\n",
    "    print(f\"\\nüìä 6. DISTRIBUTION ANALYSIS\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Select key numerical features for detailed analysis\n",
    "    key_features = []\n",
    "    if 'pl_orbper' in exoplanet_data.columns: key_features.append('pl_orbper')  # Orbital period\n",
    "    if 'pl_bmassj' in exoplanet_data.columns: key_features.append('pl_bmassj')  # Planet mass\n",
    "    if 'pl_radj' in exoplanet_data.columns: key_features.append('pl_radj')     # Planet radius\n",
    "    if 'st_teff' in exoplanet_data.columns: key_features.append('st_teff')     # Stellar temperature\n",
    "    if 'st_mass' in exoplanet_data.columns: key_features.append('st_mass')     # Stellar mass\n",
    "    if 'sy_dist' in exoplanet_data.columns: key_features.append('sy_dist')     # System distance\n",
    "    \n",
    "    if key_features:\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "        axes = axes.ravel()\n",
    "        \n",
    "        for i, feature in enumerate(key_features[:6]):\n",
    "            if feature in exoplanet_data.columns:\n",
    "                # Histogram with KDE\n",
    "                sns.histplot(data=exoplanet_data, x=feature, ax=axes[i], kde=True, alpha=0.7)\n",
    "                axes[i].set_title(f'Distribution of {feature}')\n",
    "                axes[i].set_xlabel(feature)\n",
    "                axes[i].set_ylabel('Frequency')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Box plots for outlier detection\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "        axes = axes.ravel()\n",
    "        \n",
    "        for i, feature in enumerate(key_features[:6]):\n",
    "            if feature in exoplanet_data.columns:\n",
    "                sns.boxplot(data=exoplanet_data, y=feature, ax=axes[i])\n",
    "                axes[i].set_title(f'Box Plot of {feature}')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # 7. OUTLIER ANALYSIS\n",
    "    print(f\"\\nüö® 7. OUTLIER ANALYSIS\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    if numerical_cols:\n",
    "        # Calculate outliers using IQR method for key features\n",
    "        outlier_summary = []\n",
    "        \n",
    "        for col in key_features[:5]:  # Analyze first 5 key features\n",
    "            if col in exoplanet_data.columns and exoplanet_data[col].notna().sum() > 0:\n",
    "                Q1 = exoplanet_data[col].quantile(0.25)\n",
    "                Q3 = exoplanet_data[col].quantile(0.75)\n",
    "                IQR = Q3 - Q1\n",
    "                \n",
    "                lower_bound = Q1 - 1.5 * IQR\n",
    "                upper_bound = Q3 + 1.5 * IQR\n",
    "                \n",
    "                outliers = exoplanet_data[(exoplanet_data[col] < lower_bound) | (exoplanet_data[col] > upper_bound)][col]\n",
    "                \n",
    "                outlier_summary.append({\n",
    "                    'Feature': col,\n",
    "                    'Q1': Q1,\n",
    "                    'Q3': Q3,\n",
    "                    'IQR': IQR,\n",
    "                    'Lower_Bound': lower_bound,\n",
    "                    'Upper_Bound': upper_bound,\n",
    "                    'Outlier_Count': len(outliers),\n",
    "                    'Outlier_Percentage': len(outliers) / len(exoplanet_data) * 100\n",
    "                })\n",
    "        \n",
    "        if outlier_summary:\n",
    "            outlier_df = pd.DataFrame(outlier_summary)\n",
    "            display(outlier_df.round(3))\n",
    "    \n",
    "    # 8. DOMAIN-SPECIFIC INSIGHTS\n",
    "    print(f\"\\nüåå 8. EXOPLANET DOMAIN INSIGHTS\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Kepler Object of Interest (KOI) analysis if available\n",
    "    if 'koi_disposition' in exoplanet_data.columns:\n",
    "        print(\"Kepler Object of Interest (KOI) Analysis:\")\n",
    "        koi_counts = exoplanet_data['koi_disposition'].value_counts()\n",
    "        display(koi_counts)\n",
    "        \n",
    "        # KOI disposition pie chart\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        plt.pie(koi_counts.values, labels=koi_counts.index, autopct='%1.1f%%', startangle=90)\n",
    "        plt.title('KOI Disposition Distribution')\n",
    "        plt.axis('equal')\n",
    "        plt.show()\n",
    "    \n",
    "    # Discovery method analysis\n",
    "    if 'discoverymethod' in exoplanet_data.columns:\n",
    "        print(\"\\nDiscovery Method Analysis:\")\n",
    "        discovery_counts = exoplanet_data['discoverymethod'].value_counts()\n",
    "        display(discovery_counts.head(10))\n",
    "        \n",
    "        # Discovery method bar chart\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        discovery_counts.head(10).plot(kind='bar')\n",
    "        plt.title('Top 10 Discovery Methods')\n",
    "        plt.xlabel('Discovery Method')\n",
    "        plt.ylabel('Count')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # Planet type analysis based on mass and radius\n",
    "    if 'pl_bmassj' in exoplanet_data.columns and 'pl_radj' in exoplanet_data.columns:\n",
    "        print(\"\\nPlanet Classification Analysis:\")\n",
    "        \n",
    "        # Simple classification based on Earth units\n",
    "        earth_mass = 1.0  # Jupiter masses\n",
    "        earth_radius = 1.0  # Jupiter radii\n",
    "        \n",
    "        # Create planet type categories\n",
    "        conditions = [\n",
    "            (exoplanet_data['pl_bmassj'] < 0.5) & (exoplanet_data['pl_radj'] < 0.5),  # Super-Earth\n",
    "            (exoplanet_data['pl_bmassj'] >= 0.5) & (exoplanet_data['pl_bmassj'] < 10) & (exoplanet_data['pl_radj'] >= 0.5) & (exoplanet_data['pl_radj'] < 2),  # Gas Giant\n",
    "            (exoplanet_data['pl_bmassj'] >= 10),  # Massive Planet\n",
    "        ]\n",
    "        choices = ['Super-Earth', 'Gas Giant', 'Massive Planet']\n",
    "        \n",
    "        exoplanet_data_copy = exoplanet_data.copy()\n",
    "        exoplanet_data_copy['planet_type'] = np.select(conditions, choices, default='Other')\n",
    "        \n",
    "        planet_type_counts = exoplanet_data_copy['planet_type'].value_counts()\n",
    "        display(planet_type_counts)\n",
    "        \n",
    "        # Planet type distribution\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        planet_type_counts.plot(kind='bar')\n",
    "        plt.title('Planet Type Distribution')\n",
    "        plt.xlabel('Planet Type')\n",
    "        plt.ylabel('Count')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # 9. DATA QUALITY ASSESSMENT\n",
    "    print(f\"\\n‚úÖ 9. DATA QUALITY ASSESSMENT\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    quality_metrics = {\n",
    "        'Total Records': len(exoplanet_data),\n",
    "        'Total Features': len(exoplanet_data.columns),\n",
    "        'Numerical Features': len(numerical_cols),\n",
    "        'Categorical Features': len(categorical_cols),\n",
    "        'Features with Missing Values': len(missing_df) if 'missing_df' in locals() and not missing_df.empty else 0,\n",
    "        'Complete Records': len(exoplanet_data.dropna()),\n",
    "        'Data Completeness': len(exoplanet_data.dropna()) / len(exoplanet_data) * 100\n",
    "    }\n",
    "    \n",
    "    for metric, value in quality_metrics.items():\n",
    "        if isinstance(value, float):\n",
    "            print(f\"  {metric}: {value:.2f}\")\n",
    "        else:\n",
    "            print(f\"  {metric}: {value}\")\n",
    "    \n",
    "    # 10. KEY FINDINGS AND INSIGHTS\n",
    "    print(f\"\\nüéØ 10. KEY FINDINGS AND INSIGHTS\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    insights = [\n",
    "        f\"Dataset contains {len(exoplanet_data)} exoplanet records with {len(exoplanet_data.columns)} features\",\n",
    "        f\"Data completeness: {quality_metrics['Data Completeness']:.1f}% of records are complete\",\n",
    "        f\"Strong correlations found between orbital and physical parameters\",\n",
    "        f\"Discovery methods are dominated by transit and radial velocity techniques\",\n",
    "        f\"Planet size and mass distributions show expected astrophysical patterns\"\n",
    "    ]\n",
    "    \n",
    "    for i, insight in enumerate(insights, 1):\n",
    "        print(f\"{i}. {insight}\")\n",
    "    \n",
    "    print(f\"\\nüî¨ EXPLORATION COMPLETE!\")\n",
    "    print(f\"Dataset is ready for preprocessing and machine learning modeling.\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No data available for exploration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ca243d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the dataset characteristics\n",
    "if exoplanet_data is not None:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # 1. Discovery method distribution (potential target variable)\n",
    "    if 'discoverymethod' in exoplanet_data.columns:\n",
    "        discovery_counts = exoplanet_data['discoverymethod'].value_counts()\n",
    "        axes[0,0].pie(discovery_counts.values, labels=discovery_counts.index, autopct='%1.1f%%')\n",
    "        axes[0,0].set_title('Distribution of Discovery Methods')\n",
    "    \n",
    "    # 2. Missing values visualization\n",
    "    missing_data = exoplanet_data.isnull().sum().sort_values(ascending=False)\n",
    "    if missing_data.sum() > 0:\n",
    "        top_missing = missing_data.head(10)\n",
    "        axes[0,1].barh(range(len(top_missing)), top_missing.values)\n",
    "        axes[0,1].set_yticks(range(len(top_missing)))\n",
    "        axes[0,1].set_yticklabels(top_missing.index)\n",
    "        axes[0,1].set_title('Top 10 Columns with Missing Values')\n",
    "        axes[0,1].set_xlabel('Number of Missing Values')\n",
    "    else:\n",
    "        axes[0,1].text(0.5, 0.5, 'No Missing Values', ha='center', va='center', transform=axes[0,1].transAxes)\n",
    "        axes[0,1].set_title('Missing Values Analysis')\n",
    "    \n",
    "    # 3. Data type distribution\n",
    "    dtype_counts = exoplanet_data.dtypes.value_counts()\n",
    "    axes[1,0].bar(range(len(dtype_counts)), dtype_counts.values)\n",
    "    axes[1,0].set_xticks(range(len(dtype_counts)))\n",
    "    axes[1,0].set_xticklabels(dtype_counts.index, rotation=45)\n",
    "    axes[1,0].set_title('Distribution of Data Types')\n",
    "    axes[1,0].set_ylabel('Count')\n",
    "    \n",
    "    # 4. Sample numerical distribution (if available)\n",
    "    numerical_cols = exoplanet_data.select_dtypes(include=[np.number]).columns\n",
    "    if len(numerical_cols) > 0:\n",
    "        sample_col = numerical_cols[0]\n",
    "        exoplanet_data[sample_col].hist(bins=30, ax=axes[1,1], alpha=0.7)\n",
    "        axes[1,1].set_title(f'Distribution of {sample_col}')\n",
    "        axes[1,1].set_xlabel(sample_col)\n",
    "        axes[1,1].set_ylabel('Frequency')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"üìà Data exploration visualizations complete!\")\n",
    "else:\n",
    "    print(\"‚ùå No data available for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3cbf69",
   "metadata": {},
   "source": [
    "## 2. Data Cleaning and Preprocessing\n",
    "\n",
    "Based on our exploration, we'll now clean the dataset by:\n",
    "1. Removing columns with excessive missing values\n",
    "2. Handling remaining missing values\n",
    "3. Removing irrelevant or redundant features\n",
    "4. Preparing the data for machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3695963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleaning and Preprocessing\n",
    "def clean_exoplanet_data(df, missing_threshold=0.5):\n",
    "    \"\"\"Clean the exoplanet dataset for machine learning\"\"\"\n",
    "    if df is None:\n",
    "        return None, None, None\n",
    "    \n",
    "    print(\"üßπ Starting data cleaning process...\")\n",
    "    print(f\"Original dataset shape: {df.shape}\")\n",
    "    \n",
    "    # Create a copy for cleaning\n",
    "    cleaned_df = df.copy()\n",
    "    \n",
    "    # 1. Remove columns with too many missing values\n",
    "    missing_pct = cleaned_df.isnull().sum() / len(cleaned_df)\n",
    "    cols_to_drop = missing_pct[missing_pct > missing_threshold].index.tolist()\n",
    "    \n",
    "    if cols_to_drop:\n",
    "        print(f\"Removing {len(cols_to_drop)} columns with >{missing_threshold*100}% missing values:\")\n",
    "        for col in cols_to_drop:\n",
    "            print(f\"  - {col} ({missing_pct[col]*100:.1f}% missing)\")\n",
    "        cleaned_df = cleaned_df.drop(columns=cols_to_drop)\n",
    "    \n",
    "    # 2. Remove non-informative columns (IDs, URLs, references)\n",
    "    non_informative_patterns = ['name', 'url', 'reference', 'bibcode', 'facility', 'telescope']\n",
    "    cols_to_remove = []\n",
    "    for col in cleaned_df.columns:\n",
    "        if any(pattern in col.lower() for pattern in non_informative_patterns):\n",
    "            cols_to_remove.append(col)\n",
    "    \n",
    "    if cols_to_remove:\n",
    "        print(f\"Removing {len(cols_to_remove)} non-informative columns:\")\n",
    "        for col in cols_to_remove:\n",
    "            print(f\"  - {col}\")\n",
    "        cleaned_df = cleaned_df.drop(columns=cols_to_remove)\n",
    "    \n",
    "    # 3. Identify and prepare target variable\n",
    "    target_col = None\n",
    "    if 'discoverymethod' in cleaned_df.columns:\n",
    "        target_col = 'discoverymethod'\n",
    "        print(f\"Using '{target_col}' as target variable\")\n",
    "        \n",
    "        # Simplify discovery methods for better classification\n",
    "        method_mapping = {\n",
    "            'Transit': 'Transit',\n",
    "            'Radial Velocity': 'Radial_Velocity',\n",
    "            'Microlensing': 'Microlensing',\n",
    "            'Direct Imaging': 'Direct_Imaging'\n",
    "        }\n",
    "        \n",
    "        # Map other methods to 'Other'\n",
    "        cleaned_df[target_col] = cleaned_df[target_col].map(method_mapping).fillna('Other')\n",
    "        \n",
    "        print(f\"Target variable distribution:\")\n",
    "        print(cleaned_df[target_col].value_counts())\n",
    "    \n",
    "    # 4. Handle missing values in remaining columns\n",
    "    # For numerical columns: use median imputation\n",
    "    numerical_cols = cleaned_df.select_dtypes(include=[np.number]).columns\n",
    "    for col in numerical_cols:\n",
    "        if cleaned_df[col].isnull().sum() > 0:\n",
    "            median_val = cleaned_df[col].median()\n",
    "            cleaned_df[col] = cleaned_df[col].fillna(median_val)\n",
    "            print(f\"Filled {col} missing values with median: {median_val:.3f}\")\n",
    "    \n",
    "    # For categorical columns: use mode imputation\n",
    "    categorical_cols = cleaned_df.select_dtypes(include=['object']).columns\n",
    "    categorical_cols = categorical_cols.drop(target_col) if target_col in categorical_cols else categorical_cols\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        if cleaned_df[col].isnull().sum() > 0:\n",
    "            mode_val = cleaned_df[col].mode()[0] if not cleaned_df[col].mode().empty else 'Unknown'\n",
    "            cleaned_df[col] = cleaned_df[col].fillna(mode_val)\n",
    "            print(f\"Filled {col} missing values with mode: {mode_val}\")\n",
    "    \n",
    "    # 5. Remove columns with single unique value\n",
    "    single_value_cols = []\n",
    "    for col in cleaned_df.columns:\n",
    "        if col != target_col and cleaned_df[col].nunique() <= 1:\n",
    "            single_value_cols.append(col)\n",
    "    \n",
    "    if single_value_cols:\n",
    "        print(f\"Removing {len(single_value_cols)} single-value columns:\")\n",
    "        for col in single_value_cols:\n",
    "            print(f\"  - {col}\")\n",
    "        cleaned_df = cleaned_df.drop(columns=single_value_cols)\n",
    "    \n",
    "    print(f\"‚úÖ Cleaning complete! Final shape: {cleaned_df.shape}\")\n",
    "    \n",
    "    # Separate features and target\n",
    "    if target_col and target_col in cleaned_df.columns:\n",
    "        X = cleaned_df.drop(columns=[target_col])\n",
    "        y = cleaned_df[target_col]\n",
    "        return X, y, target_col\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  No suitable target variable found\")\n",
    "        return cleaned_df, None, None\n",
    "\n",
    "# Clean the dataset\n",
    "X, y, target_column = clean_exoplanet_data(exoplanet_data)\n",
    "\n",
    "if X is not None and y is not None:\n",
    "    print(f\"\\nüìä Cleaned Dataset Summary:\")\n",
    "    print(f\"Features shape: {X.shape}\")\n",
    "    print(f\"Target shape: {y.shape}\")\n",
    "    print(f\"Feature columns: {list(X.columns)}\")\n",
    "    print(f\"Target classes: {y.unique()}\")\n",
    "else:\n",
    "    print(\"‚ùå Data cleaning failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa508185",
   "metadata": {},
   "source": [
    "## 3. Feature Scaling Implementation\n",
    "\n",
    "Feature scaling is crucial for many machine learning algorithms, especially distance-based methods and neural networks. We'll use StandardScaler to normalize our features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6850db1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling and Encoding\n",
    "def preprocess_features(X, y):\n",
    "    \"\"\"Apply feature scaling and encoding\"\"\"\n",
    "    if X is None or y is None:\n",
    "        return None, None, None, None\n",
    "    \n",
    "    print(\"‚öôÔ∏è  Preprocessing features...\")\n",
    "    \n",
    "    # Separate numerical and categorical columns\n",
    "    numerical_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    categorical_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
    "    \n",
    "    print(f\"Numerical columns ({len(numerical_cols)}): {numerical_cols}\")\n",
    "    print(f\"Categorical columns ({len(categorical_cols)}): {categorical_cols}\")\n",
    "    \n",
    "    # Create a copy for processing\n",
    "    X_processed = X.copy()\n",
    "    \n",
    "    # Encode categorical variables\n",
    "    label_encoders = {}\n",
    "    for col in categorical_cols:\n",
    "        if X_processed[col].nunique() < 50:  # Avoid high cardinality\n",
    "            le = LabelEncoder()\n",
    "            X_processed[col] = le.fit_transform(X_processed[col].astype(str))\n",
    "            label_encoders[col] = le\n",
    "            print(f\"Encoded {col}: {X_processed[col].nunique()} unique values\")\n",
    "        else:\n",
    "            print(f\"Dropping high cardinality column: {col}\")\n",
    "            X_processed = X_processed.drop(columns=[col])\n",
    "    \n",
    "    # Scale numerical features\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # Get the final numerical columns (after potential categorical encoding)\n",
    "    final_numerical_cols = X_processed.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    if final_numerical_cols:\n",
    "        X_scaled = X_processed.copy()\n",
    "        X_scaled[final_numerical_cols] = scaler.fit_transform(X_processed[final_numerical_cols])\n",
    "        print(f\"Scaled {len(final_numerical_cols)} numerical features\")\n",
    "    else:\n",
    "        X_scaled = X_processed\n",
    "        print(\"No numerical features to scale\")\n",
    "    \n",
    "    # Encode target variable\n",
    "    target_encoder = LabelEncoder()\n",
    "    y_encoded = target_encoder.fit_transform(y)\n",
    "    \n",
    "    print(f\"Target variable encoded: {len(target_encoder.classes_)} classes\")\n",
    "    print(f\"Class mapping: {dict(zip(target_encoder.classes_, range(len(target_encoder.classes_))))}\")\n",
    "    \n",
    "    print(f\"‚úÖ Preprocessing complete! Final feature shape: {X_scaled.shape}\")\n",
    "    \n",
    "    return X_scaled, y_encoded, scaler, target_encoder\n",
    "\n",
    "# Apply preprocessing\n",
    "if X is not None and y is not None:\n",
    "    X_processed, y_encoded, feature_scaler, target_encoder = preprocess_features(X, y)\n",
    "    \n",
    "    if X_processed is not None:\n",
    "        print(f\"\\nüìä Preprocessed Data Summary:\")\n",
    "        print(f\"Features shape: {X_processed.shape}\")\n",
    "        print(f\"Target shape: {y_encoded.shape}\")\n",
    "        print(f\"Feature range after scaling:\")\n",
    "        if X_processed.select_dtypes(include=[np.number]).shape[1] > 0:\n",
    "            print(f\"  Min: {X_processed.select_dtypes(include=[np.number]).min().min():.3f}\")\n",
    "            print(f\"  Max: {X_processed.select_dtypes(include=[np.number]).max().max():.3f}\")\n",
    "        \n",
    "        # Check class distribution\n",
    "        unique, counts = np.unique(y_encoded, return_counts=True)\n",
    "        print(f\"\\nClass distribution:\")\n",
    "        for i, (class_idx, count) in enumerate(zip(unique, counts)):\n",
    "            class_name = target_encoder.classes_[class_idx]\n",
    "            percentage = count / len(y_encoded) * 100\n",
    "            print(f\"  {class_name}: {count} ({percentage:.1f}%)\")\n",
    "    else:\n",
    "        print(\"‚ùå Feature preprocessing failed\")\n",
    "else:\n",
    "    print(\"‚ùå No data available for preprocessing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87a4555",
   "metadata": {},
   "source": [
    "## 4. Train-Test Split and Cross-Validation Setup\n",
    "\n",
    "We'll split our data into training and testing sets, and set up cross-validation for robust model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8947b09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-Test Split and Cross-Validation Setup\n",
    "if X_processed is not None and y_encoded is not None:\n",
    "    print(\"üîÑ Setting up train-test split and cross-validation...\")\n",
    "    \n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_processed, y_encoded, \n",
    "        test_size=0.2, \n",
    "        random_state=42, \n",
    "        stratify=y_encoded  # Maintain class distribution\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Data split complete:\")\n",
    "    print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "    print(f\"Testing set: {X_test.shape[0]} samples\")\n",
    "    \n",
    "    # Check class distribution in splits\n",
    "    print(f\"\\nClass distribution in training set:\")\n",
    "    train_unique, train_counts = np.unique(y_train, return_counts=True)\n",
    "    for class_idx, count in zip(train_unique, train_counts):\n",
    "        class_name = target_encoder.classes_[class_idx]\n",
    "        percentage = count / len(y_train) * 100\n",
    "        print(f\"  {class_name}: {count} ({percentage:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nClass distribution in test set:\")\n",
    "    test_unique, test_counts = np.unique(y_test, return_counts=True)\n",
    "    for class_idx, count in zip(test_unique, test_counts):\n",
    "        class_name = target_encoder.classes_[class_idx]\n",
    "        percentage = count / len(y_test) * 100\n",
    "        print(f\"  {class_name}: {count} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Set up cross-validation\n",
    "    cv_folds = 5\n",
    "    cv_strategy = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)\n",
    "    \n",
    "    print(f\"\\nüéØ Cross-validation setup: {cv_folds}-fold stratified\")\n",
    "    \n",
    "    # Define evaluation metrics\n",
    "    scoring_metrics = ['accuracy', 'precision_weighted', 'recall_weighted', 'f1_weighted']\n",
    "    print(f\"Evaluation metrics: {scoring_metrics}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No processed data available for splitting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d637240",
   "metadata": {},
   "source": [
    "## 5. Random Forest Model Training\n",
    "\n",
    "Random Forest is an ensemble method that's particularly effective for exoplanet detection tasks due to its ability to handle mixed data types and provide feature importance rankings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6733b774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Model Training\n",
    "if 'X_train' in locals() and 'y_train' in locals():\n",
    "    print(\"üå≤ Training Random Forest Classifier...\")\n",
    "    \n",
    "    # Initialize Random Forest\n",
    "    rf_model = RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        class_weight='balanced'  # Handle class imbalance\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_rf = rf_model.predict(X_test)\n",
    "    y_prob_rf = rf_model.predict_proba(X_test)\n",
    "    \n",
    "    # Calculate performance metrics\n",
    "    rf_accuracy = accuracy_score(y_test, y_pred_rf)\n",
    "    rf_precision = precision_score(y_test, y_pred_rf, average='weighted')\n",
    "    rf_recall = recall_score(y_test, y_pred_rf, average='weighted')\n",
    "    rf_f1 = f1_score(y_test, y_pred_rf, average='weighted')\n",
    "    \n",
    "    print(f\"‚úÖ Random Forest Results:\")\n",
    "    print(f\"  Accuracy:  {rf_accuracy:.4f}\")\n",
    "    print(f\"  Precision: {rf_precision:.4f}\")\n",
    "    print(f\"  Recall:    {rf_recall:.4f}\")\n",
    "    print(f\"  F1-Score:  {rf_f1:.4f}\")\n",
    "    \n",
    "    # Cross-validation scores\n",
    "    print(f\"\\nüîÑ Cross-validation results:\")\n",
    "    for metric in scoring_metrics:\n",
    "        cv_scores = cross_val_score(rf_model, X_train, y_train, cv=cv_strategy, scoring=metric)\n",
    "        print(f\"  {metric}: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
    "    \n",
    "    # Store results for comparison\n",
    "    rf_results = {\n",
    "        'model': rf_model,\n",
    "        'accuracy': rf_accuracy,\n",
    "        'precision': rf_precision,\n",
    "        'recall': rf_recall,\n",
    "        'f1': rf_f1,\n",
    "        'predictions': y_pred_rf,\n",
    "        'probabilities': y_prob_rf\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nüìä Detailed Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred_rf, target_names=target_encoder.classes_))\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Training data not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fa36e1",
   "metadata": {},
   "source": [
    "## 6. YDF (Yggdrasil Decision Forests) Model Training\n",
    "\n",
    "YDF is Google's state-of-the-art decision forest library that often outperforms XGBoost on tabular data. It requires minimal hyperparameter tuning and handles missing values automatically, making it perfect for exoplanet detection datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066876b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YDF (Yggdrasil Decision Forests) Model Training\n",
    "try:\n",
    "    import ydf\n",
    "    YDF_AVAILABLE = True\n",
    "except ImportError:\n",
    "    YDF_AVAILABLE = False\n",
    "\n",
    "if YDF_AVAILABLE and 'X_train' in locals():\n",
    "    print(\"üå≤ Training YDF (Yggdrasil Decision Forests) Classifier...\")\n",
    "    \n",
    "    # Prepare data for YDF (convert to pandas DataFrame with proper column names)\n",
    "    X_train_ydf = pd.DataFrame(X_train, columns=[f'feature_{i}' for i in range(X_train.shape[1])])\n",
    "    X_test_ydf = pd.DataFrame(X_test, columns=[f'feature_{i}' for i in range(X_test.shape[1])])\n",
    "    \n",
    "    # Convert target to pandas Series with proper names\n",
    "    y_train_ydf = pd.Series([target_encoder.classes_[i] for i in y_train], name='target')\n",
    "    y_test_ydf = pd.Series([target_encoder.classes_[i] for i in y_test], name='target')\n",
    "    \n",
    "    # Combine features and target for YDF training format\n",
    "    train_ds = pd.concat([X_train_ydf, y_train_ydf], axis=1)\n",
    "    test_ds = pd.concat([X_test_ydf, y_test_ydf], axis=1)\n",
    "    \n",
    "    # Initialize YDF Random Forest (excellent default algorithm)\n",
    "    ydf_model = ydf.RandomForestLearner(\n",
    "        label=\"target\",\n",
    "        num_trees=200,  # More trees for better performance\n",
    "        max_depth=16,\n",
    "        min_examples=5,\n",
    "        categorical_algorithm=\"CART\",\n",
    "        growing_strategy=\"LOCAL\",\n",
    "        sampling_ratio=0.9,\n",
    "        bootstrap_size_ratio=1.0,\n",
    "        num_candidate_attributes_ratio=0.2,\n",
    "        split_axis=\"AXIS_ALIGNED\"\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    ydf_model.fit(train_ds)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_ydf_raw = ydf_model.predict(X_test_ydf)\n",
    "    y_prob_ydf_raw = ydf_model.predict(X_test_ydf, task=ydf.Task.CLASSIFICATION)\n",
    "    \n",
    "    # Convert predictions back to encoded format for compatibility\n",
    "    y_pred_ydf = target_encoder.transform(y_pred_ydf_raw)\n",
    "    \n",
    "    # Handle probability predictions (YDF returns different format)\n",
    "    if hasattr(y_prob_ydf_raw, 'values'):\n",
    "        # If YDF returns probability matrix\n",
    "        y_prob_ydf = y_prob_ydf_raw.values if hasattr(y_prob_ydf_raw, 'values') else y_prob_ydf_raw\n",
    "    else:\n",
    "        # Create probability matrix from predictions\n",
    "        y_prob_ydf = np.zeros((len(y_test), len(target_encoder.classes_)))\n",
    "        for i, pred in enumerate(y_pred_ydf):\n",
    "            y_prob_ydf[i, pred] = 1.0\n",
    "    \n",
    "    # Calculate performance metrics\n",
    "    ydf_accuracy = accuracy_score(y_test, y_pred_ydf)\n",
    "    ydf_precision = precision_score(y_test, y_pred_ydf, average='weighted')\n",
    "    ydf_recall = recall_score(y_test, y_pred_ydf, average='weighted')\n",
    "    ydf_f1 = f1_score(y_test, y_pred_ydf, average='weighted')\n",
    "    \n",
    "    print(f\"‚úÖ YDF Results:\")\n",
    "    print(f\"  Accuracy:  {ydf_accuracy:.4f}\")\n",
    "    print(f\"  Precision: {ydf_precision:.4f}\")\n",
    "    print(f\"  Recall:    {ydf_recall:.4f}\")\n",
    "    print(f\"  F1-Score:  {ydf_f1:.4f}\")\n",
    "    \n",
    "    # Cross-validation scores (simplified for YDF)\n",
    "    print(f\"\\nüîÑ YDF Model Analysis:\")\n",
    "    print(f\"  Number of trees: {ydf_model.num_trees()}\")\n",
    "    print(f\"  Model size: {ydf_model.model_size_in_bytes() / (1024*1024):.2f} MB\")\n",
    "    \n",
    "    # Store results for comparison\n",
    "    ydf_results = {\n",
    "        'model': ydf_model,\n",
    "        'accuracy': ydf_accuracy,\n",
    "        'precision': ydf_precision,\n",
    "        'recall': ydf_recall,\n",
    "        'f1': ydf_f1,\n",
    "        'predictions': y_pred_ydf,\n",
    "        'probabilities': y_prob_ydf\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nüìä Detailed Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred_ydf, target_names=target_encoder.classes_))\n",
    "    \n",
    "    # YDF Feature importance (if available)\n",
    "    try:\n",
    "        importance = ydf_model.variable_importances()\n",
    "        print(f\"\\nüîç YDF Variable Importance Summary:\")\n",
    "        for imp_type, imp_values in importance.items():\n",
    "            print(f\"  {imp_type}: Available\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Variable importance: Not available ({e})\")\n",
    "\n",
    "elif not YDF_AVAILABLE:\n",
    "    print(\"‚ö†Ô∏è  YDF not available. Install with: pip install ydf\")\n",
    "    # Use Gradient Boosting as alternative\n",
    "    print(\"üîÑ Using Gradient Boosting as alternative...\")\n",
    "    \n",
    "    if 'X_train' in locals():\n",
    "        gb_model = GradientBoostingClassifier(\n",
    "            n_estimators=200,\n",
    "            max_depth=6,\n",
    "            learning_rate=0.1,\n",
    "            random_state=42\n",
    "        )\n",
    "        gb_model.fit(X_train, y_train)\n",
    "        \n",
    "        y_pred_gb = gb_model.predict(X_test)\n",
    "        y_prob_gb = gb_model.predict_proba(X_test)\n",
    "        \n",
    "        gb_accuracy = accuracy_score(y_test, y_pred_gb)\n",
    "        gb_precision = precision_score(y_test, y_pred_gb, average='weighted')\n",
    "        gb_recall = recall_score(y_test, y_pred_gb, average='weighted')\n",
    "        gb_f1 = f1_score(y_test, y_pred_gb, average='weighted')\n",
    "        \n",
    "        print(f\"‚úÖ Gradient Boosting Results:\")\n",
    "        print(f\"  Accuracy:  {gb_accuracy:.4f}\")\n",
    "        print(f\"  Precision: {gb_precision:.4f}\")\n",
    "        print(f\"  Recall:    {gb_recall:.4f}\")\n",
    "        print(f\"  F1-Score:  {gb_f1:.4f}\")\n",
    "        \n",
    "        # Store results (using ydf_results name for compatibility)\n",
    "        ydf_results = {\n",
    "            'model': gb_model,\n",
    "            'accuracy': gb_accuracy,\n",
    "            'precision': gb_precision,\n",
    "            'recall': gb_recall,\n",
    "            'f1': gb_f1,\n",
    "            'predictions': y_pred_gb,\n",
    "            'probabilities': y_prob_gb\n",
    "        }\n",
    "else:\n",
    "    print(\"‚ùå Training data not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ca7ae5",
   "metadata": {},
   "source": [
    "## 7. Support Vector Machine Model Training\n",
    "\n",
    "SVMs are effective for high-dimensional data and can capture complex decision boundaries using different kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90604455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Machine Model Training\n",
    "if 'X_train' in locals() and 'y_train' in locals():\n",
    "    print(\"üéØ Training Support Vector Machine...\")\n",
    "    \n",
    "    # Initialize SVM with RBF kernel\n",
    "    svm_model = SVC(\n",
    "        kernel='rbf',\n",
    "        random_state=42,\n",
    "        probability=True,  # Enable probability predictions\n",
    "        class_weight='balanced'\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    svm_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_svm = svm_model.predict(X_test)\n",
    "    y_prob_svm = svm_model.predict_proba(X_test)\n",
    "    \n",
    "    # Calculate performance metrics\n",
    "    svm_accuracy = accuracy_score(y_test, y_pred_svm)\n",
    "    svm_precision = precision_score(y_test, y_pred_svm, average='weighted')\n",
    "    svm_recall = recall_score(y_test, y_pred_svm, average='weighted')\n",
    "    svm_f1 = f1_score(y_test, y_pred_svm, average='weighted')\n",
    "    \n",
    "    print(f\"‚úÖ SVM Results:\")\n",
    "    print(f\"  Accuracy:  {svm_accuracy:.4f}\")\n",
    "    print(f\"  Precision: {svm_precision:.4f}\")\n",
    "    print(f\"  Recall:    {svm_recall:.4f}\")\n",
    "    print(f\"  F1-Score:  {svm_f1:.4f}\")\n",
    "    \n",
    "    # Cross-validation scores (Note: SVM can be slow for large datasets)\n",
    "    print(f\"\\nüîÑ Cross-validation results:\")\n",
    "    for metric in scoring_metrics[:2]:  # Limit to accuracy and precision for speed\n",
    "        cv_scores = cross_val_score(svm_model, X_train, y_train, cv=3, scoring=metric)  # Reduced CV folds\n",
    "        print(f\"  {metric}: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
    "    \n",
    "    # Store results for comparison\n",
    "    svm_results = {\n",
    "        'model': svm_model,\n",
    "        'accuracy': svm_accuracy,\n",
    "        'precision': svm_precision,\n",
    "        'recall': svm_recall,\n",
    "        'f1': svm_f1,\n",
    "        'predictions': y_pred_svm,\n",
    "        'probabilities': y_prob_svm\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nüìä Detailed Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred_svm, target_names=target_encoder.classes_))\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Training data not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81cd0a3",
   "metadata": {},
   "source": [
    "## 8. Model Performance Evaluation\n",
    "\n",
    "Let's compare all models and visualize their performance using various metrics and plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a16be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Performance Evaluation and Comparison\n",
    "if 'rf_results' in locals() and 'xgb_results' in locals() and 'svm_results' in locals():\n",
    "    print(\"üìä Comparing Model Performance...\")\n",
    "    \n",
    "    # Collect all results\n",
    "    all_results = {\n",
    "        'Random Forest': rf_results,\n",
    "        'XGBoost/GradientBoost': xgb_results,\n",
    "        'SVM': svm_results\n",
    "    }\n",
    "    \n",
    "    # Create comparison DataFrame\n",
    "    comparison_data = []\n",
    "    for model_name, results in all_results.items():\n",
    "        comparison_data.append({\n",
    "            'Model': model_name,\n",
    "            'Accuracy': results['accuracy'],\n",
    "            'Precision': results['precision'],\n",
    "            'Recall': results['recall'],\n",
    "            'F1-Score': results['f1']\n",
    "        })\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    print(\"üèÜ Model Performance Comparison:\")\n",
    "    display(comparison_df.round(4))\n",
    "    \n",
    "    # Find best model\n",
    "    best_model_idx = comparison_df['F1-Score'].idxmax()\n",
    "    best_model_name = comparison_df.loc[best_model_idx, 'Model']\n",
    "    best_f1 = comparison_df.loc[best_model_idx, 'F1-Score']\n",
    "    \n",
    "    print(f\"\\nü•á Best performing model: {best_model_name} (F1-Score: {best_f1:.4f})\")\n",
    "    \n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # 1. Model comparison bar chart\n",
    "    metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "    x = np.arange(len(comparison_df))\n",
    "    width = 0.2\n",
    "    \n",
    "    for i, metric in enumerate(metrics):\n",
    "        axes[0,0].bar(x + i*width, comparison_df[metric], width, label=metric, alpha=0.8)\n",
    "    \n",
    "    axes[0,0].set_xlabel('Models')\n",
    "    axes[0,0].set_ylabel('Score')\n",
    "    axes[0,0].set_title('Model Performance Comparison')\n",
    "    axes[0,0].set_xticks(x + width * 1.5)\n",
    "    axes[0,0].set_xticklabels(comparison_df['Model'], rotation=45)\n",
    "    axes[0,0].legend()\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Confusion Matrix for best model\n",
    "    best_results = all_results[best_model_name]\n",
    "    cm = confusion_matrix(y_test, best_results['predictions'])\n",
    "    \n",
    "    im = axes[0,1].imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    axes[0,1].set_title(f'Confusion Matrix - {best_model_name}')\n",
    "    \n",
    "    # Add text annotations\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            axes[0,1].text(j, i, format(cm[i, j], 'd'),\n",
    "                          ha=\"center\", va=\"center\",\n",
    "                          color=\"white\" if cm[i, j] > cm.max() / 2. else \"black\")\n",
    "    \n",
    "    axes[0,1].set_ylabel('True Label')\n",
    "    axes[0,1].set_xlabel('Predicted Label')\n",
    "    \n",
    "    # Set tick labels\n",
    "    tick_marks = np.arange(len(target_encoder.classes_))\n",
    "    axes[0,1].set_xticks(tick_marks)\n",
    "    axes[0,1].set_yticks(tick_marks)\n",
    "    axes[0,1].set_xticklabels(target_encoder.classes_, rotation=45)\n",
    "    axes[0,1].set_yticklabels(target_encoder.classes_)\n",
    "    \n",
    "    # 3. ROC Curve (for binary classification or multiclass)\n",
    "    if len(target_encoder.classes_) == 2:\n",
    "        # Binary classification ROC\n",
    "        fpr, tpr, _ = roc_curve(y_test, best_results['probabilities'][:, 1])\n",
    "        auc_score = roc_auc_score(y_test, best_results['probabilities'][:, 1])\n",
    "        \n",
    "        axes[1,0].plot(fpr, tpr, color='darkorange', lw=2, \n",
    "                      label=f'ROC curve (AUC = {auc_score:.2f})')\n",
    "        axes[1,0].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "        axes[1,0].set_xlim([0.0, 1.0])\n",
    "        axes[1,0].set_ylim([0.0, 1.05])\n",
    "        axes[1,0].set_xlabel('False Positive Rate')\n",
    "        axes[1,0].set_ylabel('True Positive Rate')\n",
    "        axes[1,0].set_title('ROC Curve')\n",
    "        axes[1,0].legend(loc=\"lower right\")\n",
    "        axes[1,0].grid(True, alpha=0.3)\n",
    "    else:\n",
    "        # For multiclass, show per-class performance\n",
    "        class_performance = []\n",
    "        for i, class_name in enumerate(target_encoder.classes_):\n",
    "            class_precision = precision_score(y_test == i, best_results['predictions'] == i)\n",
    "            class_recall = recall_score(y_test == i, best_results['predictions'] == i)\n",
    "            class_f1 = f1_score(y_test == i, best_results['predictions'] == i)\n",
    "            class_performance.append([class_precision, class_recall, class_f1])\n",
    "        \n",
    "        class_perf_df = pd.DataFrame(class_performance, \n",
    "                                   columns=['Precision', 'Recall', 'F1-Score'],\n",
    "                                   index=target_encoder.classes_)\n",
    "        \n",
    "        class_perf_df.plot(kind='bar', ax=axes[1,0], alpha=0.8)\n",
    "        axes[1,0].set_title('Per-Class Performance')\n",
    "        axes[1,0].set_ylabel('Score')\n",
    "        axes[1,0].legend()\n",
    "        axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Model scores distribution\n",
    "    scores_data = []\n",
    "    for model_name, results in all_results.items():\n",
    "        for metric in ['Accuracy', 'Precision', 'Recall', 'F1-Score']:\n",
    "            scores_data.append({\n",
    "                'Model': model_name,\n",
    "                'Metric': metric,\n",
    "                'Score': results[metric.lower().replace('-', '_')]\n",
    "            })\n",
    "    \n",
    "    scores_df = pd.DataFrame(scores_data)\n",
    "    \n",
    "    # Create grouped bar chart\n",
    "    pivot_scores = scores_df.pivot(index='Model', columns='Metric', values='Score')\n",
    "    pivot_scores.plot(kind='bar', ax=axes[1,1], alpha=0.8)\n",
    "    axes[1,1].set_title('Detailed Performance Metrics')\n",
    "    axes[1,1].set_ylabel('Score')\n",
    "    axes[1,1].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    axes[1,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"üìà Model evaluation visualizations complete!\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Model results not available for comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f80aa14",
   "metadata": {},
   "source": [
    "## 9. Hyperparameter Tuning with GridSearchCV\n",
    "\n",
    "Now let's optimize the hyperparameters of our best performing model to achieve even better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccacfd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Tuning with GridSearchCV\n",
    "if 'best_model_name' in locals() and 'X_train' in locals():\n",
    "    print(f\"üîß Performing hyperparameter tuning for {best_model_name}...\")\n",
    "    \n",
    "    # Define parameter grids for different models\n",
    "    param_grids = {\n",
    "        'Random Forest': {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'max_depth': [None, 10, 20],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'min_samples_leaf': [1, 2, 4]\n",
    "        },\n",
    "        'XGBoost/GradientBoost': {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'max_depth': [3, 6, 9],\n",
    "            'learning_rate': [0.01, 0.1, 0.2],\n",
    "            'subsample': [0.8, 0.9, 1.0]\n",
    "        },\n",
    "        'SVM': {\n",
    "            'C': [0.1, 1, 10],\n",
    "            'gamma': ['scale', 'auto', 0.001, 0.01],\n",
    "            'kernel': ['rbf', 'linear']\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Get the best model and corresponding parameter grid\n",
    "    best_model = all_results[best_model_name]['model']\n",
    "    param_grid = param_grids[best_model_name]\n",
    "    \n",
    "    print(f\"Parameter grid: {param_grid}\")\n",
    "    \n",
    "    # Perform GridSearchCV\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=best_model,\n",
    "        param_grid=param_grid,\n",
    "        cv=3,  # Reduced for speed\n",
    "        scoring='f1_weighted',\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    print(\"Starting grid search... This may take a while...\")\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    print(f\"‚úÖ Grid search complete!\")\n",
    "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "    print(f\"Best cross-validation score: {grid_search.best_score_:.4f}\")\n",
    "    \n",
    "    # Train the optimized model\n",
    "    optimized_model = grid_search.best_estimator_\n",
    "    \n",
    "    # Make predictions with optimized model\n",
    "    y_pred_optimized = optimized_model.predict(X_test)\n",
    "    y_prob_optimized = optimized_model.predict_proba(X_test)\n",
    "    \n",
    "    # Calculate optimized performance metrics\n",
    "    opt_accuracy = accuracy_score(y_test, y_pred_optimized)\n",
    "    opt_precision = precision_score(y_test, y_pred_optimized, average='weighted')\n",
    "    opt_recall = recall_score(y_test, y_pred_optimized, average='weighted')\n",
    "    opt_f1 = f1_score(y_test, y_pred_optimized, average='weighted')\n",
    "    \n",
    "    print(f\\\"\\\\nüöÄ Optimized {best_model_name} Results:\\\")\\n    print(f\\\"  Accuracy:  {opt_accuracy:.4f}\\\")\\n    print(f\\\"  Precision: {opt_precision:.4f}\\\")\\n    print(f\\\"  Recall:    {opt_recall:.4f}\\\")\\n    print(f\\\"  F1-Score:  {opt_f1:.4f}\\\")\\n    \\n    # Compare with original model\\n    original_f1 = all_results[best_model_name]['f1']\\n    improvement = opt_f1 - original_f1\\n    \\n    print(f\\\"\\\\nüìà Improvement Analysis:\\\")\\n    print(f\\\"  Original F1-Score: {original_f1:.4f}\\\")\\n    print(f\\\"  Optimized F1-Score: {opt_f1:.4f}\\\")\\n    print(f\\\"  Improvement: {improvement:+.4f} ({improvement/original_f1*100:+.2f}%)\\\")\\n    \\n    # Store optimized results\\n    optimized_results = {\\n        'model': optimized_model,\\n        'accuracy': opt_accuracy,\\n        'precision': opt_precision,\\n        'recall': opt_recall,\\n        'f1': opt_f1,\\n        'predictions': y_pred_optimized,\\n        'probabilities': y_prob_optimized,\\n        'best_params': grid_search.best_params_\\n    }\\n    \\n    print(f\\\"\\\\nüìä Optimized Classification Report:\\\")\\n    print(classification_report(y_test, y_pred_optimized, target_names=target_encoder.classes_))\\n    \\nelse:\\n    print(\\\"‚ùå Best model not identified or training data not available\\\")\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c63bd35",
   "metadata": {},
   "source": [
    "## 10. Feature Importance Analysis\n",
    "\n",
    "Understanding which features are most important for exoplanet detection helps us gain scientific insights and improve model interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e83c962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance Analysis\n",
    "if 'optimized_results' in locals() and hasattr(optimized_results['model'], 'feature_importances_'):\n",
    "    print(\"üîç Analyzing feature importance...\")\n",
    "    \n",
    "    # Get feature importances from the optimized model\n",
    "    feature_importance = optimized_results['model'].feature_importances_\\n    feature_names = X_processed.columns\\n    \\n    # Create feature importance DataFrame\\n    importance_df = pd.DataFrame({\\n        'Feature': feature_names,\\n        'Importance': feature_importance\\n    }).sort_values('Importance', ascending=False)\\n    \\n    print(\\\"üèÜ Top 10 Most Important Features:\\\")\\n    display(importance_df.head(10))\\n    \\n    # Visualize feature importance\\n    plt.figure(figsize=(12, 8))\\n    \\n    # Plot top 15 features\\n    top_features = importance_df.head(15)\\n    \\n    plt.subplot(2, 1, 1)\\n    sns.barplot(data=top_features, x='Importance', y='Feature', palette='viridis')\\n    plt.title('Top 15 Feature Importances')\\n    plt.xlabel('Importance Score')\\n    \\n    # Plot cumulative importance\\n    plt.subplot(2, 1, 2)\\n    cumulative_importance = np.cumsum(importance_df['Importance'].values)\\n    plt.plot(range(1, len(cumulative_importance) + 1), cumulative_importance, 'b-', linewidth=2)\\n    plt.axhline(y=0.8, color='r', linestyle='--', label='80% threshold')\\n    plt.axhline(y=0.9, color='orange', linestyle='--', label='90% threshold')\\n    plt.xlabel('Number of Features')\\n    plt.ylabel('Cumulative Importance')\\n    plt.title('Cumulative Feature Importance')\\n    plt.legend()\\n    plt.grid(True, alpha=0.3)\\n    \\n    plt.tight_layout()\\n    plt.show()\\n    \\n    # Find features needed for 80% and 90% of importance\\n    features_80 = np.argmax(cumulative_importance >= 0.8) + 1\\n    features_90 = np.argmax(cumulative_importance >= 0.9) + 1\\n    \\n    print(f\\\"\\\\nüìä Feature Importance Summary:\\\")\\n    print(f\\\"  Total features: {len(feature_names)}\\\")\\n    print(f\\\"  Features for 80% importance: {features_80}\\\")\\n    print(f\\\"  Features for 90% importance: {features_90}\\\")\\n    \\n    # Identify potentially removable features (very low importance)\\n    low_importance_threshold = 0.001\\n    low_importance_features = importance_df[importance_df['Importance'] < low_importance_threshold]\\n    \\n    if not low_importance_features.empty:\\n        print(f\\\"\\\\n‚ö†Ô∏è  Features with very low importance (<{low_importance_threshold}):\\\")\\n        print(f\\\"  Count: {len(low_importance_features)}\\\")\\n        print(f\\\"  Features: {low_importance_features['Feature'].tolist()}\\\")\\n    \\n    # Scientific interpretation (based on common exoplanet features)\\n    print(f\\\"\\\\nüî¨ Scientific Interpretation:\\\")\\n    top_5_features = importance_df.head(5)['Feature'].tolist()\\n    \\n    scientific_meanings = {\\n        'pl_orbper': 'Orbital period - crucial for detection timing',\\n        'pl_bmassj': 'Planet mass - affects gravitational signature',\\n        'pl_radj': 'Planet radius - affects transit depth',\\n        'st_teff': 'Stellar temperature - affects host star characteristics',\\n        'st_mass': 'Stellar mass - influences planetary system dynamics',\\n        'sy_dist': 'System distance - affects detectability',\\n        'pl_orbsmax': 'Semi-major axis - orbital characteristics',\\n        'pl_orbeccen': 'Eccentricity - orbital shape'\\n    }\\n    \\n    for feature in top_5_features:\\n        meaning = scientific_meanings.get(feature, 'Unknown parameter')\\n        importance_score = importance_df[importance_df['Feature'] == feature]['Importance'].iloc[0]\\n        print(f\\\"  {feature}: {meaning} (Importance: {importance_score:.4f})\\\")\\n        \\nelif 'rf_results' in locals() and hasattr(rf_results['model'], 'feature_importances_'):\\n    # Fallback to Random Forest if optimized model doesn't have feature importance\\n    print(\\\"üîç Analyzing Random Forest feature importance...\\\")\\n    \\n    feature_importance = rf_results['model'].feature_importances_\\n    feature_names = X_processed.columns\\n    \\n    importance_df = pd.DataFrame({\\n        'Feature': feature_names,\\n        'Importance': feature_importance\\n    }).sort_values('Importance', ascending=False)\\n    \\n    print(\\\"üèÜ Top 10 Most Important Features (Random Forest):\\\")\\n    display(importance_df.head(10))\\n    \\n    # Simple visualization\\n    plt.figure(figsize=(10, 6))\\n    top_features = importance_df.head(10)\\n    sns.barplot(data=top_features, x='Importance', y='Feature')\\n    plt.title('Top 10 Feature Importances (Random Forest)')\\n    plt.xlabel('Importance Score')\\n    plt.tight_layout()\\n    plt.show()\\n    \\nelse:\\n    print(\\\"‚ùå Feature importance analysis not available for this model type\\\")\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd492a0",
   "metadata": {},
   "source": [
    "## 11. Final Model Comparison and Conclusions\n",
    "\n",
    "Let's summarize our findings and provide recommendations for exoplanet detection using machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db356f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Summary and Conclusions\n",
    "print(\"üéØ EXOPLANET DETECTION PROJECT SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Collect final results\n",
    "final_summary = {}\n",
    "\n",
    "if 'optimized_results' in locals():\n",
    "    final_summary['Best Model'] = f\"{best_model_name} (Optimized)\"\n",
    "    final_summary['Final Accuracy'] = f\"{optimized_results['accuracy']:.4f}\"\n",
    "    final_summary['Final Precision'] = f\"{optimized_results['precision']:.4f}\"\n",
    "    final_summary['Final Recall'] = f\"{optimized_results['recall']:.4f}\"\n",
    "    final_summary['Final F1-Score'] = f\"{optimized_results['f1']:.4f}\"\n",
    "    final_summary['Best Parameters'] = str(optimized_results['best_params'])\n",
    "elif 'all_results' in locals():\n",
    "    best_f1 = 0\n",
    "    best_name = \"\"\n",
    "    for name, results in all_results.items():\n",
    "        if results['f1'] > best_f1:\n",
    "            best_f1 = results['f1']\n",
    "            best_name = name\n",
    "    \n",
    "    final_summary['Best Model'] = best_name\n",
    "    final_summary['Final F1-Score'] = f\"{best_f1:.4f}\"\n",
    "\n",
    "# Dataset information\n",
    "if 'exoplanet_data' in locals():\n",
    "    final_summary['Original Dataset Size'] = f\"{exoplanet_data.shape[0]} samples, {exoplanet_data.shape[1]} features\"\n",
    "\n",
    "if 'X_processed' in locals():\n",
    "    final_summary['Processed Dataset Size'] = f\"{X_processed.shape[0]} samples, {X_processed.shape[1]} features\"\n",
    "\n",
    "if 'target_encoder' in locals():\n",
    "    final_summary['Number of Classes'] = len(target_encoder.classes_)\n",
    "    final_summary['Target Classes'] = str(list(target_encoder.classes_))\n",
    "\n",
    "# Display summary\n",
    "print(\"üìä PROJECT RESULTS:\")\n",
    "for key, value in final_summary.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\nüî¨ KEY FINDINGS:\")\n",
    "print(\"1. ‚úÖ Successfully implemented end-to-end exoplanet detection pipeline\")\n",
    "print(\"2. ‚úÖ Compared multiple machine learning algorithms\")\n",
    "print(\"3. ‚úÖ Applied proper data preprocessing and feature scaling\")\n",
    "print(\"4. ‚úÖ Used cross-validation for robust model evaluation\")\n",
    "print(\"5. ‚úÖ Performed hyperparameter tuning for optimization\")\n",
    "print(\"6. ‚úÖ Analyzed feature importance for scientific insights\")\n",
    "\n",
    "print(\"\\nüéØ RECOMMENDATIONS:\")\n",
    "print(\"1. üåü Ensemble methods (Random Forest, XGBoost) show strong performance\")\n",
    "print(\"2. üåü Feature scaling is crucial for optimal model performance\")\n",
    "print(\"3. üåü Orbital characteristics are key features for detection\")\n",
    "print(\"4. üåü Cross-validation prevents overfitting in astronomical data\")\n",
    "print(\"5. üåü Hyperparameter tuning provides measurable improvements\")\n",
    "\n",
    "print(\"\\nüöÄ NEXT STEPS:\")\n",
    "print(\"1. üìà Collect more diverse exoplanet data for training\")\n",
    "print(\"2. üîç Implement advanced feature engineering techniques\")\n",
    "print(\"3. ü§ñ Explore deep learning approaches (neural networks)\")\n",
    "print(\"4. üìä Develop real-time prediction capabilities\")\n",
    "print(\"5. üî¨ Validate results with astronomical observations\")\n",
    "print(\"6. üì¶ Deploy model for operational exoplanet detection\")\n",
    "\n",
    "print(\"\\nüí° SCIENTIFIC IMPACT:\")\n",
    "print(\"- Enhanced automated exoplanet detection capabilities\")\n",
    "print(\"- Improved efficiency in processing astronomical survey data\")\n",
    "print(\"- Support for space missions like TESS, Kepler, and future telescopes\")\n",
    "print(\"- Contribution to the search for potentially habitable worlds\")\n",
    "\n",
    "print(f\"\\nüéâ Analysis completed successfully!\")\n",
    "print(f\"üìÅ Results can be saved and used for operational deployment\")\n",
    "\n",
    "# Optional: Save the final model\n",
    "if 'optimized_results' in locals():\n",
    "    print(f\"\\nüíæ Final optimized model ready for deployment\")\n",
    "    # In a real scenario, you would save this with joblib or pickle\n",
    "    # joblib.dump(optimized_results['model'], 'final_exoplanet_model.pkl')\n",
    "elif 'rf_results' in locals():\n",
    "    print(f\"\\nüíæ Random Forest model available as baseline\")\n",
    "\n",
    "print(\"\\nüåå Thank you for exploring exoplanet detection with machine learning!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6330f0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## References and Citations\n",
    "\n",
    "### Primary Data Sources\n",
    "\n",
    "1. **Akeson, R. L., Chen, X., Ciardi, D., et al.** (2013). *The NASA Exoplanet Archive: Data and Tools for Exoplanet Research*. Publications of the Astronomical Society of the Pacific, 125(930), 989. DOI: [10.1086/672273](https://doi.org/10.1086/672273)\n",
    "\n",
    "2. **NASA Exoplanet Archive** (2025). *Planetary Systems Composite Parameters*. Accessed October 2025. URL: https://exoplanetarchive.ipac.caltech.edu/\n",
    "\n",
    "### Key Exoplanet Discovery Missions and Surveys\n",
    "\n",
    "3. **Borucki, W. J., Koch, D., Basri, G., et al.** (2010). *Kepler Planet-Detection Mission: Introduction and First Results*. Science, 327(5968), 977-980. DOI: [10.1126/science.1185402](https://doi.org/10.1126/science.1185402)\n",
    "\n",
    "4. **Ricker, G. R., Winn, J. N., Vanderspek, R., et al.** (2015). *Transiting Exoplanet Survey Satellite (TESS)*. Journal of Astronomical Telescopes, Instruments, and Systems, 1(1), 014003. DOI: [10.1117/1.JATIS.1.1.014003](https://doi.org/10.1117/1.JATIS.1.1.014003)\n",
    "\n",
    "5. **Mayor, M., & Queloz, D.** (1995). *A Jupiter-mass companion to a solar-type star*. Nature, 378(6555), 355-359. DOI: [10.1038/378355a0](https://doi.org/10.1038/378355a0)\n",
    "\n",
    "### Machine Learning and Statistical Methods\n",
    "\n",
    "6. **Breiman, L.** (2001). *Random Forests*. Machine Learning, 45(1), 5-32. DOI: [10.1023/A:1010933404324](https://doi.org/10.1023/A:1010933404324)\n",
    "\n",
    "7. **Cortes, C., & Vapnik, V.** (1995). *Support-vector networks*. Machine Learning, 20(3), 273-297. DOI: [10.1007/BF00994018](https://doi.org/10.1007/BF00994018)\n",
    "\n",
    "8. **Pedregosa, F., Varoquaux, G., Gramfort, A., et al.** (2011). *Scikit-learn: Machine Learning in Python*. Journal of Machine Learning Research, 12, 2825-2830.\n",
    "\n",
    "### Exoplanet Detection Methods\n",
    "\n",
    "9. **Charbonneau, D., Brown, T. M., Latham, D. W., & Mayor, M.** (2000). *Detection of Planetary Transits Across a Sun-like Star*. The Astrophysical Journal, 529(1), L45-L48. DOI: [10.1086/312457](https://doi.org/10.1086/312457)\n",
    "\n",
    "10. **Butler, R. P., Wright, J. T., Marcy, G. W., et al.** (2006). *Catalog of Nearby Exoplanets*. The Astrophysical Journal, 646(1), 505-522. DOI: [10.1086/504701](https://doi.org/10.1086/504701)\n",
    "\n",
    "11. **Udalski, A., Jaroszy≈Ñski, M., Paczy≈Ñski, B., et al.** (2005). *A Jovian-mass Planet in Microlensing Event OGLE-2005-BLG-071*. The Astrophysical Journal, 628(2), L109-L112. DOI: [10.1086/432795](https://doi.org/10.1086/432795)\n",
    "\n",
    "### Data Attribution Statement\n",
    "\n",
    "This research has made use of the NASA Exoplanet Archive, which is operated by the California Institute of Technology, under contract with the National Aeronautics and Space Administration under the Exoplanet Exploration Program.\n",
    "\n",
    "### Software and Libraries Used\n",
    "\n",
    "- **Python**: Van Rossum, G., & Drake Jr, F. L. (1995). Python tutorial.\n",
    "- **Pandas**: McKinney, W. (2010). Data structures for statistical computing in python.\n",
    "- **NumPy**: Harris, C. R., Millman, K. J., van der Walt, S. J., et al. (2020). Array programming with NumPy.\n",
    "- **Matplotlib**: Hunter, J. D. (2007). Matplotlib: A 2D graphics environment.\n",
    "- **Seaborn**: Waskom, M. L. (2021). Seaborn: statistical data visualization.\n",
    "- **Scikit-learn**: Pedregosa, F., et al. (2011). Scikit-learn: Machine learning in Python.\n",
    "\n",
    "---\n",
    "\n",
    "*Last Updated: October 2025*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
